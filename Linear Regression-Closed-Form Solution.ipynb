{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Linear Regression: Closed-Form Solution\n",
    "\n",
    "\n",
    "There are two very different solution approaches for the Linear Regression problem.\n",
    "    - The “closed-form” solution approach.\n",
    "    - Iterative optimization approach known as Gradient Descent (GD).\n",
    "    \n",
    "\n",
    "\n",
    "In this notebook we investigate the implementation of the **closed-form** solution for the Linear Regression problem.\n",
    "\n",
    "The “closed-form” solution directly computes the model parameters (weights) that best fit the model to the training set. I.e., the model parameters that minimize the cost function over the training set.\n",
    "\n",
    "\n",
    "### Three Scenarios\n",
    "\n",
    "Depending on the type of the data matrix X, following three scenarios may arise in the closed-form solution approach:\n",
    "    - Scenario 1: Data Matrix X is Square\n",
    "    - Scenario 2: Data Matrix X is Non-Square (No Collinearity)\n",
    "    - Scenario 3: Data Matrix X is Non-Square (Collinearity)\n",
    "\n",
    "For the last two scenarios we need to use a special closed-form technique known as the **Ordinary Least Squares (OLS)** method.\n",
    "\n",
    "\n",
    "### Summary of the Solution Approach for three Scenarios\n",
    "\n",
    "- Scenario 1: Data Matrix X is Square\n",
    "\n",
    "        We use a simple Linear Algebra trick (iverting the data matrix) to derive the closed-form solution.\n",
    "\n",
    "\n",
    "- Scenario 2: Data Matrix X is Non-Square (No Collinearity)\n",
    "        \n",
    "        We implement a special technique called the Ordinary Least Squares (OLS) method to derive the closed-form solution.\n",
    "\n",
    "\n",
    "- Scenario 3: Data Matrix X is Non-Square (Collinearity)\n",
    "\n",
    "        We implementat a regularized (penalized) least square method or Ridge regression method to derive the closed-form solution.\n",
    "\n",
    "\n",
    "\n",
    "#### <font color=red>Note: </font>\n",
    "We implement the OLS method (for scenario 2 & 3) using only the **NumPy** library. It helps to understand the structure of the OLS solution. There is also a **Scikit-Learn implementation** of the OLS method for the Linear Regression problem. We will use that solution in a different notebook. This notebook does not use any Scikit-Learn solutions.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, det, matrix_rank\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Useful Linear Algebra Techniques Using NumPy\n",
    "\n",
    "Before we perform the investigation, we review some basic Linear Algebra techniques for:\n",
    "- Representation of a feature/weight vector using a numpy array\n",
    "- Computing dot product between the feature vectors and the weight vectors\n",
    "\n",
    "\n",
    "\n",
    "## Creating the Target by Using Feature and the Weight\n",
    "\n",
    "The prediction or target vector (\"y\") is created by taking the dot product between the feature vector (denoted by lower case \"x\") and the weight vector (denoted by \"w\").\n",
    "\n",
    "We will discuss two scenarios for computing the dot product:\n",
    "1. A single feature vector (d-dimensional)\n",
    "2. Multiple feature vectors (d-dimensional)\n",
    "\n",
    "###### Note that a data matrix (denoted by upper case \"X\") is constructed by using multiple d-dimensiional feature vectors.\n",
    "\n",
    "\n",
    "## Dot Product between a Single Feature Vector ($\\vec x$) and the Weight Vector ($\\vec w$)\n",
    "\n",
    "We compute the dot product between the feature vector $\\vec x$ and the weight vector $\\vec w$ in one of the following two ways:\n",
    "\n",
    "1. Method 1: Create the two vectors as **column vectors**, then take the transpose of either $\\vec x$ or $\\vec w$. Compute the dot product between them.\n",
    "2. Method 2: Create the two vectors as **row vectors**. Then, compute the dot product. Don't need to compute transpose.\n",
    "\n",
    "\n",
    "#### <font color=red>Method 1 is often used.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Create the two vectors as column vectors, then take the transpose of either x or w.  Compute the dot product between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[ 1]\n",
      " [ 5]\n",
      " [10]]\n",
      "w:\n",
      " [[5]\n",
      " [6]\n",
      " [1]]\n",
      "\n",
      "Transpose of w: [[5 6 1]]\n",
      "Transpose of x: [[ 1  5 10]]\n",
      "\n",
      "Dot Product of x and w:\n",
      "y1 (x^T.w): [[45]]\n",
      "y2 (w^T.x): [[45]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Define a single sample \"x\" with 3 features as a column vector\n",
    "Define a 3D weight vector \"w\" as a column vector\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# sample x with 3D feature is defined as a column vector\n",
    "x = np.array([1, 5, 10]).reshape(-1, 1)\n",
    "\n",
    "# 3D weight vector is defined as a column vector \n",
    "w = np.array([5, 6, 1]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "print(\"x:\\n\", x)\n",
    "print(\"w:\\n\", w)\n",
    "\n",
    "\n",
    "print(\"\\nTranspose of w:\", w.T)\n",
    "print(\"Transpose of x:\", x.T)\n",
    "\n",
    "\n",
    "print(\"\\nDot Product of x and w:\")\n",
    "\n",
    "\n",
    "y1 = x.T.dot(w)\n",
    "print(\"y1 (x^T.w):\", y1)\n",
    "\n",
    "\n",
    "y2 = w.T.dot(x)\n",
    "print(\"y2 (w^T.x):\", y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Define the two vectors as row vectors. Then, compute the dot product. \n",
    "\n",
    "#### <font color=red>Don't need to compute transpose.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [5 6 1]\n",
      "x: [ 1  5 10]\n",
      "\n",
      "Dot Product of x and w:\n",
      "\n",
      "h1 (w.x): 45\n",
      "\n",
      "h2 (x.w): 45\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define a single sample \"x\" with 3 features as a row vector\n",
    "Define a 3D weight vector \"w\" as a row vector\n",
    "'''\n",
    "\n",
    "w = np.array([5, 6, 1])\n",
    "x = np.array([1, 5, 10])\n",
    "\n",
    "\n",
    "print(\"w:\", w)\n",
    "print(\"x:\", x)\n",
    "\n",
    "\n",
    "print(\"\\nDot Product of x and w:\")\n",
    "\n",
    "y1 = w.dot(x)\n",
    "print(\"\\nh1 (w.x):\", y1)\n",
    "\n",
    "\n",
    "y2 = x.dot(w)\n",
    "print(\"\\nh2 (x.w):\", y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product Between Data Matrix X ($N$ Features) and the Weight Vector ($\\vec w$)\n",
    "\n",
    "A data matrix is denoted by upper case \"X\". \n",
    "\n",
    "It is created by transposing the \"N\" feature vectors (d-dimensional colmun vectors) and adding them as rows (number of rows = N).\n",
    "\n",
    "Each row of X represents a data point (transposed feature vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [[5]\n",
      " [6]\n",
      " [1]]\n",
      "\n",
      "X:\n",
      " [[ 1  5 10]\n",
      " [ 1  4  9]\n",
      " [ 1  3 13]\n",
      " [ 1  2  6]]\n",
      "\n",
      "Note: Each row of X represents a transposed feature vector.\n",
      "\n",
      "y:\n",
      " [[45]\n",
      " [38]\n",
      " [36]\n",
      " [23]]\n"
     ]
    }
   ],
   "source": [
    "# Define the 3D weight vector as a column vector\n",
    "w = np.array([5, 6, 1]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# The data matrix \"X\" is created by transposing the \"N\" feature vectors (d-dimensional colmun vectors) \n",
    "#   and adding them as rows (number of rows = N)\n",
    "# The dimension of this matrix is N x d\n",
    "X = np.array([[1, 5, 10],\n",
    "              [1, 4, 9],\n",
    "              [1, 3, 13],\n",
    "              [1, 2, 6]\n",
    "             ])\n",
    "\n",
    "print(\"w:\\n\", w)\n",
    "print(\"\\nX:\\n\", X)\n",
    "print(\"\\nNote: Each row of X represents a transposed feature vector.\")\n",
    "\n",
    "# The target vector y is created by taking the dot product between X and w.\n",
    "# Note: we don't have to apply transpose as we dis in case a single feature\n",
    "y = X.dot(w)\n",
    "print(\"\\ny:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Linear Regression Problem\n",
    "\n",
    "The Linear Regression problem is modeled by a system of linear equations or linear system.\n",
    "- $\\vec X. \\vec w = \\vec y$\n",
    "\n",
    "The goal is to learn the unknown weight vector ($\\vec w$) of a linear system.\n",
    "\n",
    "Depending on the dimension of the data Matrix X, the following two scenarios arise:\n",
    "- Data Matrix X is Square\n",
    "- Data Matrix X is Non-Square\n",
    "\n",
    "For square X, the solution is trivial.\n",
    "\n",
    "However, when X is non-square, we need to use a special method called the **Ordinary Least Squares (OLS)**. \n",
    "\n",
    "\n",
    "Below we solve the linear system for the above two scenarios by finding the unknown weight vector $\\vec w$, then using it, we predict the target vector. Finally, we evaluate the prediction performance by using the following two evaluation metrics. \n",
    "\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "We will use two evaluation metrics.\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Coefficient of Determination ($R^2$ or $r^2$)\n",
    "\n",
    "\n",
    "### Note on $R^2$:\n",
    "R-squared is a statistical measure of **how close the data are to the fitted regression line**. \n",
    "\n",
    "R-squared metric measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "$R^2 = \\frac{Explained Variation}{Total Variation}$\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "\n",
    "#### <font color=red>In general, the higher the R-squared value, the better the model fits your data.</font>\n",
    "\n",
    "\n",
    "#### Compute $R^2$ using the sklearn:\n",
    "\n",
    "- The \"r2_score\" function from sklearn.metrics\n",
    "\n",
    "#### Compute MSE using the sklearn:\n",
    "\n",
    "- The \"mean_squared_error\" function from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix X is Square\n",
    "\n",
    "When the data matrix X is square, the OLS solution is given by:\n",
    "\n",
    " $\\vec w = \\vec X^{-1} . \\vec y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X:\n",
      " [[13 22 35]\n",
      " [17 13  9]\n",
      " [ 7  3 12]\n",
      " [ 1 31  1]]\n",
      "\n",
      "y:\n",
      " [22 34 45 67]\n",
      "\n",
      "Weight Vector:  [63.96483756 -1.49554279  0.1703645  -0.75059429]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 0.00\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression: Square Data Matrix\n",
    "\n",
    "\n",
    "# Create a data matrix that becomes a square matrix after adding the bias term \n",
    "X = np.array([[13, 22, 35],\n",
    "              [17, 13, 9],\n",
    "              [7, 3, 12],\n",
    "              [1, 31, 1]])\n",
    "\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors\n",
    "X_bias = np.c_[np.ones((X.shape[0],1)),X]\n",
    "\n",
    "\n",
    "# Create the target vector\n",
    "y = np.array([22, 34, 45, 67])\n",
    "\n",
    "\n",
    "# OLS solution\n",
    "w = np.linalg.inv(X_bias).dot(y)\n",
    "\n",
    "print(\"\\nX:\\n\", X)\n",
    "print(\"\\ny:\\n\", y)\n",
    "\n",
    "print(\"\\nWeight Vector: \", w)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction \n",
    "y_predicted = X_bias.dot(w)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, y_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix X is Non-Square\n",
    "\n",
    "If the data matrix X is not square, then we need to solve the **Normal equation**, instead of the $\\vec X. \\vec w = \\vec y$ linear system of equations.\n",
    "\n",
    "The solution to the Normal equation is known as the Ordinary Least Squares (OLS) solution.\n",
    "\n",
    "Below we implement the OLS method for Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS) Method\n",
    "\n",
    "\n",
    "The Ordinary Least Squares (OLS) is a type of linear least squares method for estimating the unknown parameters ($\\vec w$) in a linear regression model. We can find the optimal weight vectors of linear regression by using the OLS method.\n",
    "\n",
    "The OLS method is used to solve a linear system of equations defined by the **Normal Equation.**\n",
    "\n",
    "##### Normal Equation:  $X^TXw = X^TY$\n",
    "\n",
    "The OLS solution to the Normal equation is: $\\hat w_{OLS} = (X^TX)^{-1}X^TY$\n",
    "\n",
    "Therefore, given the non-square data matrix X and the true target vector Y, we can find $\\hat w_{OLS}$.\n",
    "\n",
    "\n",
    "### Note: \n",
    "The OLS solution **<font color=red>doesn't work</font>** if the columns of data matrix X has colinearity. In this case, we cannot compute $(X^TX)^{-1}$ as it becomes **singular**.\n",
    "\n",
    "Thus, we consider two cases for non-square data matrix X:\n",
    "- Non-Square X: No Collinearity in the Columns\n",
    "- Non-Square X: Collinearity in the Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Square X: No Collinearity in the Columns\n",
    "\n",
    "We create a data matrix X that doesn't have collinearity in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X:\n",
      " [[13 22 35]\n",
      " [17 13  9]]\n",
      "\n",
      "y:\n",
      " [22 34]\n",
      "\n",
      "Rank of X:  2\n",
      "\n",
      "Rank of X_bias:  2\n",
      "\n",
      "Determinant of (X_bias^T.X_bias):  -4.633503900004057e-24\n",
      "\n",
      "Rank of z:  2\n",
      "\n",
      "Weight vector:\n",
      " [-24.625        4.51171875   1.12695312  -0.84179688]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 343.29\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: -8.54\n"
     ]
    }
   ],
   "source": [
    "# OLS Method for Linear Regression\n",
    "\n",
    "\n",
    "# Create a data matrix that doesn't have collinearity in the columns\n",
    "X = np.array([[13, 22, 35],\n",
    "              [17, 13, 9]])\n",
    "\n",
    "# Create the target vector\n",
    "y = np.array([22, 34])\n",
    "\n",
    "\n",
    "print(\"\\nX:\\n\", X)\n",
    "print(\"\\ny:\\n\", y)\n",
    "\n",
    "\n",
    "print(\"\\nRank of X: \", matrix_rank(X))\n",
    "\n",
    "# Add a bias term with the feature vectors\n",
    "X_bias = np.c_[np.ones((X.shape[0],1)),X]\n",
    "print(\"\\nRank of X_bias: \", matrix_rank(X_bias))\n",
    "\n",
    "\n",
    "# The determinant should be non-zero\n",
    "print(\"\\nDeterminant of (X_bias^T.X_bias): \", det(X_bias.T.dot(X_bias)))\n",
    "\n",
    "\n",
    "# Computes the product of the transpose of X_bias with itself\n",
    "z = X_bias.T.dot(X_bias) \n",
    "\n",
    "print(\"\\nRank of z: \", matrix_rank(z))\n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "w = np.linalg.inv(z).dot(X_bias.T).dot(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nWeight vector:\\n\",w)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction (you should use test data)\n",
    "y_predicted = X_bias.dot(w)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, y_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Square X: Collinearity in the Columns\n",
    "\n",
    "Note that the OLS solution **<font color=red>doesn't work</font>** if the columns of data matrix X has collinearity. In this case, we cannot compute $(X^TX)^{-1}$ as it becomes **singular**.\n",
    "\n",
    "Below we will create a data matrix X that has collinearity in its columns. We will see that $(X^TX)^{-1}$ suffers from **singularity**.\n",
    "\n",
    "To verify the singularity of $(X^TX)^{-1}$, we compute its eigenvalues. If all eigenvalues are not positive, then this matrx will **not be positive definite**, consequently singular.\n",
    "\n",
    "We will see that $(X^TX)^{-1}$ is not positive definitie (i.e., not all of its eigenvalues are positive). As a consequence, the matrix is not invertible.\n",
    "\n",
    "After investigating possible singularity in $(X^TX)^{-1}$, we implement a technique for fixing the singularity problem of $(X^TX)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X (N x d):\n",
      "\n",
      "[[ 1  2  3  4]\n",
      " [ 7 14 11 12]]\n",
      "\n",
      "Rank of X:  2\n",
      "\n",
      "Z (= X^T.X):\n",
      "\n",
      "[[ 50 100  80  88]\n",
      " [100 200 160 176]\n",
      " [ 80 160 130 144]\n",
      " [ 88 176 144 160]]\n",
      "\n",
      "Rank of Z:  2\n",
      "\n",
      "Determinant of Z (X^T.X):  0.0\n",
      "\n",
      "Eigenvalues:\n",
      "\n",
      "[ 5.36563313e+02  3.43668670e+00 -2.40578862e-14 -1.15269744e-14]\n",
      "\n",
      "Number of zero/negative eigenvalues: 2\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6cbed6feb798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Invert X^T.X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mZ_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nInverse of X^T.X (d x d):\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hasan/anaconda/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hasan/anaconda/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Create a matrix \"X\" N x d in which N >> d.\n",
    "# Some columns of X are linearly dependent.\n",
    "# Then, compute \"X^T.X\" and see if it's invertible.\n",
    "\n",
    "\n",
    "# Create a non-square matrix & make a column linearly dependent on another column\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [7, 14, 11, 12]])\n",
    "\n",
    "\n",
    "#This matrix doesn't have singularity (you can use this for experimentation)\n",
    "# X = np.array([[1, 13, 22, 35],\n",
    "#               [1, 17, 13, 1]])\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nX (N x d):\\n\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nRank of X: \", matrix_rank(X))\n",
    "\n",
    "\n",
    "# Compute X^T.X\n",
    "Z = X.T.dot(X)\n",
    "print(\"\\nZ (= X^T.X):\\n\")\n",
    "print(Z)\n",
    "print(\"\\nRank of Z: \", matrix_rank(Z))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nDeterminant of Z (X^T.X): \", det(Z))\n",
    "\n",
    "# Find the eigenvalues of X^T.X\n",
    "# For this particular example (X), not all eigenvalues will be positive\n",
    "# Hence, X^T.X is not positive definite & is NOT invertible\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(Z)\n",
    "\n",
    "print(\"\\nEigenvalues:\\n\")\n",
    "print(eigenvalues)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of zero/negative eigenvalues\n",
    "noOfZeroEigenvalues = 0\n",
    "for i in range(len(eigenvalues)):\n",
    "    if (eigenvalues[i] > 0):\n",
    "        noOfZeroEigenvalues = 0\n",
    "    else:\n",
    "        noOfZeroEigenvalues += 1\n",
    "        \n",
    "print(\"\\nNumber of zero/negative eigenvalues: %d\" % noOfZeroEigenvalues)\n",
    "\n",
    "\n",
    "# Invert X^T.X\n",
    "Z_inv = np.linalg.inv(Z)\n",
    "print(\"\\nInverse of X^T.X (d x d):\\n\")\n",
    "print(Z_inv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the Singularity Problem of $(X^TX)^{-1}$\n",
    "\n",
    "We have observed the $(X^TX)^{-1}$ in the previous example was **NOT positive definitie**, and hence was singular.\n",
    "\n",
    "One intuitive trick to solve the singularity problem of $(X^TX)^{-1}$ is to add some positive small numbers (known as penalty) on its diagonal. It will make its eigenvalues positive.\n",
    "\n",
    "Later we will see that this is a **Bayesian** solution to the non-invertibility problem of $(X^TX)^{-1}$d.\n",
    "\n",
    "This Bayesian solution is known as **<font color=blue> Ridge Regression. </font>** or regularized (penalized) least square method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (N x d):\n",
      "[[ 1  2  3  4]\n",
      " [ 7 14 11 12]]\n",
      "\n",
      "Rank of X:  2\n",
      "\n",
      "Z (= X^T.X):\n",
      "[[ 50 100  80  88]\n",
      " [100 200 160 176]\n",
      " [ 80 160 130 144]\n",
      " [ 88 176 144 160]]\n",
      "\n",
      "Determinant of Z (X^T.X):  0.0\n",
      "\n",
      "Eigenvalues of Z:\n",
      "[ 5.36563313e+02  3.43668670e+00 -2.40578862e-14 -1.15269744e-14]\n",
      "\n",
      "Number of zero/negative eigenvalues: 2\n",
      "\n",
      "-------- Fixing the Singularity of (X^T)X ------------\n",
      "\n",
      "Diagonal Matrix:\n",
      "[[0.001 0.    0.    0.   ]\n",
      " [0.    0.001 0.    0.   ]\n",
      " [0.    0.    0.001 0.   ]\n",
      " [0.    0.    0.    0.001]]\n",
      "\n",
      "Z + Diagonal Matrix:\n",
      "[[ 50.001 100.     80.     88.   ]\n",
      " [100.    200.001 160.    176.   ]\n",
      " [ 80.    160.    130.001 144.   ]\n",
      " [ 88.    176.    144.    160.001]]\n",
      "\n",
      "Eigenvalues of (Z + Diagonal):\n",
      "[5.36564313e+02+0.0000000e+00j 3.43768670e+00+0.0000000e+00j\n",
      " 1.00000000e-03+9.3634816e-15j 1.00000000e-03-9.3634816e-15j]\n",
      "\n",
      "Number of zero/negative eigenvalues: 0\n",
      "\n",
      "Determinant of Z (X^T.X) after adding the diagonal matrix:  0.0018445400010404084\n",
      "\n",
      "Inverse of X^T.X:\n",
      "[[ 806.97084379 -386.05831243  -69.43736646   43.32353863]\n",
      " [-386.05831243  227.88337513 -138.87473292   86.64707727]\n",
      " [ -69.43736647 -138.87473291  694.1622303  -433.79053832]\n",
      " [  43.32353864   86.64707727 -433.79053832  271.27630776]]\n"
     ]
    }
   ],
   "source": [
    "# Create a non-square matrix & make a column linearly dependent on another column\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [7, 14, 11, 12]])\n",
    "\n",
    "\n",
    "print(\"X (N x d):\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nRank of X: \", matrix_rank(X))\n",
    "\n",
    "# Compute X^T.X\n",
    "Z = np.transpose(X).dot(X)\n",
    "print(\"\\nZ (= X^T.X):\")\n",
    "print(Z)\n",
    "\n",
    "\n",
    "print(\"\\nDeterminant of Z (X^T.X): \", det(Z))\n",
    "\n",
    "\n",
    "\n",
    "# Find the eigenvalues of X^T.X\n",
    "# For this particular example (X), some eigenvalues may not be positive\n",
    "# Hence, X^T.X is not positive definite & is NOT invertible\n",
    "\n",
    "eigenValues, eigenVectors = np.linalg.eig(Z)\n",
    "\n",
    "print(\"\\nEigenvalues of Z:\")\n",
    "print(eigenValues)\n",
    "\n",
    "\n",
    "# Count the number of zero/negative eigenvalues\n",
    "noOfZeroEigenvalues = 0\n",
    "for i in range(len(eigenvalues)):\n",
    "    if (eigenvalues[i] > 0):\n",
    "        noOfZeroEigenvalues = 0\n",
    "    else:\n",
    "        noOfZeroEigenvalues += 1\n",
    "        \n",
    "print(\"\\nNumber of zero/negative eigenvalues: %d\" % noOfZeroEigenvalues)\n",
    "\n",
    "\n",
    "# Invert X^T.X\n",
    "# Z_inv = np.linalg.inv(Z)\n",
    "# print(\"\\nInverse of X^T.X (d x d):\\n\")\n",
    "# print(Z_inv)\n",
    "\n",
    "\n",
    "print(\"\\n-------- Fixing the Singularity of (X^T)X ------------\")\n",
    "\n",
    "\n",
    "# Create a digonal matrix with small positive numbers on the diagonal\n",
    "diagonal = np.array([[0.001, 0, 0, 0],\n",
    "                     [0, 0.001, 0, 0],\n",
    "                     [0, 0, 0.001, 0],\n",
    "                     [0, 0, 0, 0.001]])\n",
    "\n",
    "\n",
    "print(\"\\nDiagonal Matrix:\")\n",
    "print(diagonal)\n",
    "\n",
    "\n",
    "# Add the diogonal matrix with Z\n",
    "print(\"\\nZ + Diagonal Matrix:\")\n",
    "Z = Z + diagonal\n",
    "\n",
    "print(Z)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(Z)\n",
    "\n",
    "print(\"\\nEigenvalues of (Z + Diagonal):\")\n",
    "print(eigenvalues)\n",
    "\n",
    "\n",
    "# Count the number of zero/negative eigenvalues\n",
    "noOfZeroEigenvalues = 0\n",
    "for i in range(len(eigenvalues)):\n",
    "    if (eigenvalues[i] > 0):\n",
    "        noOfZeroEigenvalues = 0\n",
    "    else:\n",
    "        noOfZeroEigenvalues += 1\n",
    "        \n",
    "print(\"\\nNumber of zero/negative eigenvalues: %d\" % noOfZeroEigenvalues)\n",
    "\n",
    "print(\"\\nDeterminant of Z (X^T.X) after adding the diagonal matrix: \", det(Z))\n",
    "\n",
    "# Invert X^T.X\n",
    "Z_inv = np.linalg.inv(Z)\n",
    "print(\"\\nInverse of X^T.X:\")\n",
    "print(Z_inv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying OLS Method on Data Matrix With Collinearity in Columns\n",
    "\n",
    "Now that we know how to fix the singularity problem of the $(X^T)X$ matrix, we can apply the OLS method on a data matrix that has collinearity in its columns.\n",
    "\n",
    "Note that the singularity problem can be solved by adding small positive numbers on the diagonal of the $(X^T)X$ matrix.\n",
    "\n",
    "We will see later that this technique is not ad-hoc. It is a natural consequence of applying the Bayesian technique on the OLS. The Bayesian technique is known as the **Ridge Regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_bias^T.X_bias):  0.0\n",
      "\n",
      "-------- Fixing the Singularity of (X_bias^T)X_bias ------------\n",
      "The diagonal matrix:\n",
      " [[0.001 0.    0.    0.   ]\n",
      " [0.    0.001 0.    0.   ]\n",
      " [0.    0.    0.001 0.   ]\n",
      " [0.    0.    0.    0.001]]\n",
      "\n",
      "The weight vector:\n",
      " [0.10522057 0.83835579 1.67671159 0.5058005 ]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 0.00\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data matrix X that creates singularity for (X^T)X \n",
    "\n",
    "X = np.array([[1, 2, 35],\n",
    "              [7, 14, 9]])\n",
    "\n",
    "y = np.array([22, 34])\n",
    "\n",
    "# Add a bias term with the feature vectors\n",
    "X_bias = np.c_[np.ones((X.shape[0],1)),X]\n",
    "\n",
    "\n",
    "# The determinant should be zero as X_bias^T.X_bias is singular\n",
    "print(\"\\nDeterminant of (X_bias^T.X_bias): \", det(X_bias.T.dot(X_bias)))\n",
    "\n",
    "\n",
    "\n",
    "# Computes the product of the transpose of X_bias with itself\n",
    "z = X_bias.T.dot(X_bias) \n",
    "\n",
    "\n",
    "print(\"\\n-------- Fixing the Singularity of (X_bias^T)X_bias ------------\")\n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z\n",
    "diagonal = np.eye(z.shape[0], dtype=float)\n",
    "\n",
    "# Add small non-zero numbers on the diagonal\n",
    "diagonal = diagonal * 0.001\n",
    "print(\"The diagonal matrix:\\n\", diagonal)\n",
    "\n",
    "# Closed form solution for weight vector w using Ridge Regregression\n",
    "w = np.linalg.inv(z + diagonal).dot(X_bias.T).dot(y)\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\",w)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction (you should use test data)\n",
    "y_predicted = X_bias.dot(w)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, y_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y, y_predicted))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
